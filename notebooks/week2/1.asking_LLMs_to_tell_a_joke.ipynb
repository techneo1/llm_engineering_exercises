{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0740e390",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "947eb224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3281f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key not set\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f56aa10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9798e9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You're an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3e6b921",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bb74c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the statistician invited to the party? \n",
      "\n",
      "Because they always bring a good distribution!\n"
     ]
    }
   ],
   "source": [
    "# Use OpenAI's GPT-3.5 Turbo model to generate a response\n",
    "completion = openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113c44bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do data scientists prefer dark mode?\n",
      "\n",
      "Because light attracts bugs!\n"
     ]
    }
   ],
   "source": [
    "# Use OpenAI's GPT-4o-mini model to generate a response\n",
    "completion = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4977fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they heard the project was on another level!\n"
     ]
    }
   ],
   "source": [
    "completion = openai.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=prompts,\n",
    "    temperature=0.4,\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1321fc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one for the data science crowd:\n",
      "\n",
      "Why did the data scientist become a gardener?\n",
      "\n",
      "They heard it was the best way to work with root mean squares! \n",
      "\n",
      "*Ba dum tss* ü•Å\n",
      "\n",
      "Alternative joke if you'd like another:\n",
      "\n",
      "What's a data scientist's favorite kind of joke?\n",
      "A SQL! (sequel)\n"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-5-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6a907f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one for the data scientists:\n",
      "\n",
      "Why did the data scientist become a gardener?\n",
      "\n",
      "Because they heard they could really grow their ROOTs! \n",
      "\n",
      "*ba dum tss* \n",
      "\n",
      "(This works on multiple levels since ROOT is both a popular data analysis framework and, well, plants have roots! Plus data scientists love looking for growth opportunities üòÑ)\n",
      "\n",
      "Want another one?"
     ]
    }
   ],
   "source": [
    "#Claude 3.5 Sonnet Streaming\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-5-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3f9b2a",
   "metadata": {},
   "source": [
    "## Back to OpenAI with a serious question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eee0c086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Deciding if a business problem is suitable for a Large Language Model (LLM) solution involves several considerations. Below are the steps and factors to evaluate:\n",
       "\n",
       "### 1. Define the Problem Clearly\n",
       "- **Objective Clarity**: Is the problem well-defined with clear objectives?\n",
       "- **Complexity**: Is the problem complex enough to require advanced language understanding?\n",
       "\n",
       "### 2. Evaluate the Nature of the Problem\n",
       "- **Textual Data**: Does the problem involve processing, generating, or understanding large volumes of text?\n",
       "- **Language Understanding**: Does it require nuanced language comprehension, such as sentiment analysis, summarization, or translation?\n",
       "- **Conversational AI**: Is there a need for human-like interaction, such as in chatbots or virtual assistants?\n",
       "\n",
       "### 3. Consider Data Availability and Quality\n",
       "- **Data Volume**: Do you have sufficient data to train or fine-tune an LLM?\n",
       "- **Data Relevance**: Is the data relevant and high-quality, covering the necessary linguistic and contextual nuances?\n",
       "\n",
       "### 4. Assess Feasibility and Resources\n",
       "- **Computational Resources**: Do you have the infrastructure to support the computational demands of an LLM?\n",
       "- **Budget Constraints**: Can you afford the costs associated with LLM implementation, including data preparation, training, and deployment?\n",
       "- **Expertise**: Does your team have the necessary expertise to implement and maintain an LLM solution?\n",
       "\n",
       "### 5. Understand the Limitations of LLMs\n",
       "- **Bias and Ethics**: Are you prepared to handle potential biases and ethical considerations in LLM outputs?\n",
       "- **Real-Time Processing**: Can your application tolerate the latency of LLM responses, especially if real-time processing is required?\n",
       "\n",
       "### 6. Evaluate Business Value and ROI\n",
       "- **Impact**: Will an LLM solution significantly improve efficiency, accuracy, or customer satisfaction?\n",
       "- **Cost-Benefit Analysis**: Does the potential return on investment justify the cost of implementing an LLM?\n",
       "\n",
       "### 7. Explore Alternatives\n",
       "- **Simpler Solutions**: Are there simpler, rule-based, or traditional machine learning solutions that could solve the problem effectively?\n",
       "- **Hybrid Approaches**: Could a combination of methods (e.g., LLMs for text generation and traditional ML for classification) be more effective?\n",
       "\n",
       "### Conclusion\n",
       "If after evaluating these factors, you find that:\n",
       "- The problem is inherently language-driven.\n",
       "- There's ample high-quality data.\n",
       "- The business impact is significant.\n",
       "- You have the resources and expertise.\n",
       "- The cost is justified by potential benefits.\n",
       "\n",
       "Then, an LLM solution might be suitable for your business problem. Otherwise, consider alternative approaches or refine your problem definition and constraints."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]\n",
    "\n",
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
