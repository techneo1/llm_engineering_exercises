{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Jupyter!\n"
     ]
    }
   ],
   "source": [
    "# Say hello to Jupyter\n",
    "print('Hello Jupyter!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv('GROQ_API_KEY')\n",
    "print(api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n"
     ]
    }
   ],
   "source": [
    "# First interaction with Llama using Groq Serverless API\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('GROQ_API_KEY')\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "message = \"Hello, what is your name?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    messages=[{\"role\": \"user\", \"content\": message}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Static webisite scraper\n",
    "\n",
    "# A class to represent a Webpage\n",
    "\n",
    "# Some websites need you to use proper headers when fetching them:\n",
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "class Website:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        \"\"\"\n",
    "        Create this Website object from the given url using the BeautifulSoup library\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home - Edward Donner\n",
      "Home\n",
      "Connect Four\n",
      "Outsmart\n",
      "An arena that pits LLMs against each other in a battle of diplomacy and deviousness\n",
      "About\n",
      "Posts\n",
      "Well, hi there.\n",
      "I’m Ed. I like writing code and experimenting with LLMs, and hopefully you’re here because you do too. I also enjoy DJing (but I’m badly out of practice), amateur electronic music production (\n",
      "very\n",
      "amateur) and losing myself in\n",
      "Hacker News\n",
      ", nodding my head sagely to things I only half understand.\n",
      "I’m the co-founder and CTO of\n",
      "Nebula.io\n",
      ". We’re applying AI to a field where it can make a massive, positive impact: helping people discover their potential and pursue their reason for being. Recruiters use our product today to source, understand, engage and manage talent. I’m previously the founder and CEO of AI startup untapt,\n",
      "acquired in 2021\n",
      ".\n",
      "We work with groundbreaking, proprietary LLMs verticalized for talent, we’ve\n",
      "patented\n",
      "our matching model, and our award-winning platform has happy customers and tons of press coverage.\n",
      "Connect\n",
      "with me for more!\n",
      "January 23, 2025\n",
      "LLM Workshop – Hands-on with Agents – resources\n",
      "December 21, 2024\n",
      "Welcome, SuperDataScientists!\n",
      "November 13, 2024\n",
      "Mastering AI and LLM Engineering – Resources\n",
      "October 16, 2024\n",
      "From Software Engineer to AI Data Scientist – resources\n",
      "Navigation\n",
      "Home\n",
      "Connect Four\n",
      "Outsmart\n",
      "An arena that pits LLMs against each other in a battle of diplomacy and deviousness\n",
      "About\n",
      "Posts\n",
      "Get in touch\n",
      "ed [at] edwarddonner [dot] com\n",
      "www.edwarddonner.com\n",
      "Follow me\n",
      "LinkedIn\n",
      "Twitter\n",
      "Facebook\n",
      "Subscribe to newsletter\n",
      "Type your email…\n",
      "Subscribe\n"
     ]
    }
   ],
   "source": [
    "ed = Website(\"https://edwarddonner.com\")\n",
    "print(ed.title)\n",
    "print(ed.text)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Summary of Edward Donner's Website\n",
       "The website belongs to Edward Donner, a co-founder and CTO of Nebula.io, who is passionate about writing code, experimenting with Large Language Models (LLMs), and electronic music production. He has experience as the founder and CEO of AI startup untapt, which was acquired in 2021.\n",
       "\n",
       "### News and Announcements\n",
       "Some recent updates and resources shared on the website include:\n",
       "* LLM Workshop – Hands-on with Agents (January 23, 2025)\n",
       "* Welcome message for SuperDataScientists (December 21, 2024)\n",
       "* Mastering AI and LLM Engineering – Resources (November 13, 2024)\n",
       "* From Software Engineer to AI Data Scientist – resources (October 16, 2024)\n",
       "\n",
       "### About the Author\n",
       "Edward Donner is involved in applying AI to help people discover their potential and pursue their reason for being. His company, Nebula.io, uses proprietary LLMs and a patented matching model to source, understand, engage, and manage talent."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "system_prompt = \"You are an assistant that analyzes the contents of a website \\\n",
    "and provides a short summary, ignoring text that might be navigation related. \\\n",
    "Respond in markdown.\"\n",
    "\n",
    "def user_prompt_for(website):\n",
    "    user_prompt = f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt += \"\\nThe contents of this website is as follows; \\\n",
    "please provide a short summary of this website in markdown. \\\n",
    "If it includes news or announcements, then summarize these too.\\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    return user_prompt\n",
    "\n",
    "# print(user_prompt_for(ed))\n",
    "\n",
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
    "    ]\n",
    "\n",
    "# print(messages_for(ed))\n",
    "\n",
    "def summarize(url):\n",
    "    website = Website(url)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=messages_for(website)\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# summarize(\"https://edwarddonner.com\")\n",
    "\n",
    "def display_summary(url):\n",
    "    summary = summarize(url)\n",
    "    display(Markdown(summary))\n",
    "\n",
    "display_summary(\"https://edwarddonner.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Website Summary\n",
       "The website appears to be incomplete or unavailable as it only displays a message prompting the user to **enable JavaScript and cookies** to continue. There is no notable content, news, or announcements present on the page."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_summary(\"https://openai.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Overview of OpenAI Website\n",
       "The OpenAI website provides an overview of the company's products and services, including ChatGPT, Sora, and API Platform. The website is divided into sections such as Research, Safety, and For Business, showcasing the company's focus on AI research, safety, and business applications.\n",
       "\n",
       "### Latest News and Announcements\n",
       "Some of the latest news and announcements on the website include:\n",
       "* Introducing NextGenAI (March 4, 2025)\n",
       "* OpenAI and Guardian Media Group launch content partnership (February 14, 2025)\n",
       "* Introducing the Intelligence Age (February 9, 2025)\n",
       "* Introducing data residency in Europe (February 5, 2025)\n",
       "* OpenAI and the CSU system bring AI to 500,000 students & faculty (February 4, 2025)\n",
       "\n",
       "### Products and Services\n",
       "The website highlights the following products and services:\n",
       "* ChatGPT: a conversational AI model\n",
       "* Sora: an AI model for generating images and text\n",
       "* API Platform: a platform for developers to build AI applications\n",
       "\n",
       "### Research and Safety\n",
       "The website emphasizes OpenAI's commitment to research and safety, with sections dedicated to:\n",
       "* Research Index: a list of research publications and projects\n",
       "* Safety Approach: an overview of the company's approach to safety and security\n",
       "* Security & Privacy: information on the company's security and privacy policies\n",
       "\n",
       "### Business and Education\n",
       "The website also provides information on business and education applications, including:\n",
       "* For Business: an overview of OpenAI's business solutions and partnerships\n",
       "* Education: resources for educators and students using OpenAI's products and services"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dynamic webisite scraper\n",
    "\n",
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "import nest_asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "class Website:\n",
    "    title: str\n",
    "    text: str\n",
    "    url: str\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "         \n",
    "    async def run(self, playwright):\n",
    "        browser = await playwright.chromium.launch(headless=False)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(self.url)\n",
    "        await page.wait_for_load_state('load')\n",
    "        \n",
    "        # Extract data from the page\n",
    "        self.title = await page.title()\n",
    "        text = await page.content()\n",
    "        await browser.close()\n",
    "    \n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "        for irrelevant in soup([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "    \n",
    "    async def main(self):\n",
    "        async with async_playwright() as playwright:\n",
    "            await self.run(playwright)   \n",
    "\n",
    "def display_dynamic_site_summary(url):\n",
    "    if __name__ == \"__main__\":\n",
    "        site = Website(url)\n",
    "        asyncio.run(site.main())\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            messages=messages_for(site)\n",
    "        )\n",
    "\n",
    "        web_summary = response.choices[0].message.content\n",
    "        display(Markdown(web_summary))\n",
    "\n",
    "display_dynamic_site_summary('https://openai.com')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
